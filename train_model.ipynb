{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT MODULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you run requirements.txt to import all the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from time import time\n",
    "from emoji import demojize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class defines how the data is handled throught the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, filename, label_col='label', text_col='text'):\n",
    "        self.filename = filename\n",
    "        self.label_col = label_col\n",
    "        self.text_col = text_col\n",
    "\n",
    "    def data(self):\n",
    "        data = self.dataframe[[self.label_col, self.text_col]].copy()\n",
    "        data.columns = ['label', 'text']\n",
    "        return data\n",
    "\n",
    "    def cleaned_data(self):\n",
    "        data =  self.dataframe[[self.label_col, 'cleaned']]\n",
    "        data.columns = ['label', 'text']\n",
    "        return data\n",
    "\n",
    "    def load(self):\n",
    "        df = pd.read_csv(Path(self.filename).resolve())\n",
    "        self.dataframe = df\n",
    "\n",
    "    def preprocess_texts(self, quiet=False):\n",
    "        self.dataframe['cleaned'] = preprocess(self.dataframe[self.text_col], quiet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lowercasing the post content\n",
    "2. Remove hyperlinks \n",
    "3. Converting emojis to text\n",
    "4. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, quiet=False):\n",
    "    texts = texts.str.lower()\n",
    "    texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "    texts = texts.apply(demojize)\n",
    "    texts = texts.str.replace(r\"::\", \": :\")\n",
    "    texts = texts.str.replace(r\"â€™\", \"'\")\n",
    "    texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    texts = texts.str.replace(pattern, r\"\\1\")\n",
    "    texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "    texts = texts.str.replace(r\"n't\", ' not')\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.remove('not')\n",
    "    stopwords.remove('nor')\n",
    "    stopwords.remove('no')\n",
    "    texts = texts.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "    print(\"Preprocessing done\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON formatted train file is converted to CSV format with the columns - id, label and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_file(f):\n",
    "    file = open(f)\n",
    "    data = json.load(file)\n",
    "    df = pd.DataFrame(columns=[\"id\",\"label\",\"text\"])\n",
    "    for key in data:\n",
    "        emotions = data[key][\"emotion\"]\n",
    "        for emo,value in emotions.items():\n",
    "            if(value == True):\n",
    "                df = df.append({\"id\":key,\"label\":emo,\"text\":data[key][\"body\"]}, ignore_index = True)\n",
    "    df.to_csv(\"nlp_train.csv\")\n",
    "    \n",
    "#Change the name of the JSON Train file accordingly\n",
    "train_file(\"nlp_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>answering question criticism individual referr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgust</td>\n",
       "      <td>answering question criticism individual referr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pessimism</td>\n",
       "      <td>answering question criticism individual referr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>i'm going start today's discussion thread pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anticipation</td>\n",
       "      <td>i'm going start today's discussion thread pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text\n",
       "0         anger  answering question criticism individual referr...\n",
       "1       disgust  answering question criticism individual referr...\n",
       "2     pessimism  answering question criticism individual referr...\n",
       "3         anger  i'm going start today's discussion thread pers...\n",
       "4  anticipation  i'm going start today's discussion thread pers..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Project path is added to the ENV VARIABLE - PYTHONPATH\n",
    "sys.path.append(Path(os.path.join(os.path.abspath(''), '../')).resolve().as_posix())\n",
    "dataset_path = Path('nlp_train.csv').resolve()\n",
    "dataset = Dataset(dataset_path)\n",
    "dataset.load()\n",
    "dataset.preprocess_texts()\n",
    "cleaned_df = dataset.cleaned_data()\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text words are converted into word vectors and saved to \"tokenizer.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = cleaned_df['text'].str.split().str.len().sum()\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True)\n",
    "tokenizer.fit_on_texts(cleaned_df.text)\n",
    "file_to_save = Path('tokenizer.pickle').resolve()\n",
    "with file_to_save.open('wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN, TEST AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data is also split here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  import sys\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(columns=['label', 'text'])\n",
    "validation = pd.DataFrame(columns=['label', 'text'])\n",
    "test = pd.DataFrame(columns=['label','test'])\n",
    "for label in cleaned_df.label.unique():\n",
    "    label_data = cleaned_df[cleaned_df.label == label]\n",
    "    train_data, validation_data = train_test_split(label_data, test_size=0.2)\n",
    "    train = pd.concat([train, train_data])\n",
    "    validation = pd.concat([validation, validation_data])\n",
    "\n",
    "test.to_csv(\"nlp_test.csv\") #Test data created to test the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEURAL MODEL  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Stacked Bidirectional LSTM, CNN model is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 100, 500)     13523000    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, 100, 500)     0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 100, 256)     644096      spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 100, 256)     394240      bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 98, 64)       49216       bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 12)           1548        concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 14,612,100\n",
      "Trainable params: 14,612,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "num_classes = len(cleaned_df.label.unique())\n",
    "embedding_dim = 500\n",
    "input_length = 100\n",
    "lstm_units = 128\n",
    "lstm_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "spatial_dropout=0.2\n",
    "filters=64\n",
    "kernel_size=3\n",
    "\n",
    "input_layer = Input(shape=(input_length,))\n",
    "output_layer = Embedding(\n",
    "  input_dim=input_dim,\n",
    "  output_dim=embedding_dim,\n",
    "  input_shape=(input_length,)\n",
    ")(input_layer)\n",
    "\n",
    "output_layer = SpatialDropout1D(spatial_dropout)(output_layer)\n",
    "\n",
    "output_layer = Bidirectional(\n",
    "LSTM(lstm_units, return_sequences=True,\n",
    "     dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)\n",
    ")(output_layer)\n",
    "\n",
    "output_layer = Bidirectional(\n",
    "LSTM(lstm_units, return_sequences=True,\n",
    "     dropout=lstm_dropout, recurrent_dropout=recurrent_dropout)\n",
    ")(output_layer)\n",
    "\n",
    "output_layer = Conv1D(filters, kernel_size=kernel_size, padding='valid',\n",
    "                    kernel_initializer='glorot_uniform')(output_layer)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(output_layer)\n",
    "max_pool = GlobalMaxPooling1D()(output_layer)\n",
    "output_layer = concatenate([avg_pool, max_pool])\n",
    "\n",
    "output_layer = Dense(num_classes, activation='softmax')(output_layer)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODING & TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [text.split() for text in train.text]\n",
    "validation_sequences = [text.split() for text in validation.text]\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_sequences)\n",
    "list_tokenized_validation = tokenizer.texts_to_sequences(validation_sequences)\n",
    "x_train = pad_sequences(list_tokenized_train, maxlen=input_length)\n",
    "x_validation = pad_sequences(list_tokenized_validation, maxlen=input_length)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(cleaned_df.label.unique())\n",
    "\n",
    "encoder_path = Path('../', 'encoder.pickle')\n",
    "with encoder_path.open('wb') as file:\n",
    "    pickle.dump(encoder, file)\n",
    "\n",
    "y_train = encoder.transform(train.label)\n",
    "y_validation = encoder.transform(validation.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4080 samples, validate on 516 samples\n",
      "Epoch 1/25\n",
      "4080/4080 [==============================] - 57s 14ms/sample - loss: 2.3750 - acc: 0.1301 - val_loss: 2.3410 - val_acc: 0.1589\n",
      "Epoch 2/25\n",
      "4080/4080 [==============================] - 55s 14ms/sample - loss: 2.2874 - acc: 0.1576 - val_loss: 2.3029 - val_acc: 0.1550\n",
      "Epoch 3/25\n",
      "4080/4080 [==============================] - 55s 13ms/sample - loss: 2.1393 - acc: 0.2056 - val_loss: 2.3508 - val_acc: 0.1182\n",
      "Epoch 4/25\n",
      "4080/4080 [==============================] - 59s 15ms/sample - loss: 2.0064 - acc: 0.2495 - val_loss: 2.4941 - val_acc: 0.1105\n",
      "Epoch 5/25\n",
      "4080/4080 [==============================] - 57s 14ms/sample - loss: 1.9188 - acc: 0.2390 - val_loss: 2.6041 - val_acc: 0.0504\n",
      "Epoch 6/25\n",
      "4080/4080 [==============================] - 57s 14ms/sample - loss: 1.8419 - acc: 0.2547 - val_loss: 2.7463 - val_acc: 0.0349\n",
      "Epoch 7/25\n",
      "4080/4080 [==============================] - 58s 14ms/sample - loss: 1.7743 - acc: 0.2635 - val_loss: 2.9421 - val_acc: 0.0271\n",
      "Epoch 8/25\n",
      "4080/4080 [==============================] - 55s 13ms/sample - loss: 1.7179 - acc: 0.2674 - val_loss: 3.0695 - val_acc: 0.0213\n",
      "Epoch 9/25\n",
      "4080/4080 [==============================] - 54s 13ms/sample - loss: 1.6823 - acc: 0.2507 - val_loss: 3.2680 - val_acc: 0.0174\n",
      "Epoch 10/25\n",
      "4080/4080 [==============================] - 60s 15ms/sample - loss: 1.6432 - acc: 0.2586 - val_loss: 3.4162 - val_acc: 0.0155\n",
      "Epoch 11/25\n",
      "4080/4080 [==============================] - 56s 14ms/sample - loss: 1.6071 - acc: 0.2439 - val_loss: 3.5939 - val_acc: 0.0174\n",
      "Epoch 12/25\n",
      "4080/4080 [==============================] - 55s 14ms/sample - loss: 1.5767 - acc: 0.2505 - val_loss: 3.6334 - val_acc: 0.0174\n",
      "Epoch 13/25\n",
      "4080/4080 [==============================] - 58s 14ms/sample - loss: 1.5567 - acc: 0.2392 - val_loss: 3.8325 - val_acc: 0.0174\n",
      "Epoch 14/25\n",
      "4080/4080 [==============================] - 58s 14ms/sample - loss: 1.5401 - acc: 0.2451 - val_loss: 3.7957 - val_acc: 0.0174\n",
      "Epoch 15/25\n",
      "4080/4080 [==============================] - 59s 15ms/sample - loss: 1.5384 - acc: 0.2449 - val_loss: 3.9192 - val_acc: 0.0136\n",
      "Epoch 16/25\n",
      "4080/4080 [==============================] - 57s 14ms/sample - loss: 1.5192 - acc: 0.2596 - val_loss: 3.9823 - val_acc: 0.0213\n",
      "Epoch 17/25\n",
      "4080/4080 [==============================] - 58s 14ms/sample - loss: 1.5159 - acc: 0.2542 - val_loss: 4.0939 - val_acc: 0.0155\n",
      "Epoch 18/25\n",
      "4080/4080 [==============================] - 58s 14ms/sample - loss: 1.4945 - acc: 0.2480 - val_loss: 4.2168 - val_acc: 0.0213\n",
      "Epoch 19/25\n",
      "4080/4080 [==============================] - 56s 14ms/sample - loss: 1.4868 - acc: 0.2500 - val_loss: 4.1374 - val_acc: 0.0213\n",
      "Epoch 20/25\n",
      "4080/4080 [==============================] - 57s 14ms/sample - loss: 1.4739 - acc: 0.2466 - val_loss: 4.3467 - val_acc: 0.0174\n",
      "Epoch 21/25\n",
      "4080/4080 [==============================] - 55s 14ms/sample - loss: 1.4780 - acc: 0.2576 - val_loss: 4.2854 - val_acc: 0.0194\n",
      "Epoch 22/25\n",
      "4080/4080 [==============================] - 55s 13ms/sample - loss: 1.4716 - acc: 0.2600 - val_loss: 4.3596 - val_acc: 0.0174\n",
      "Epoch 23/25\n",
      "4080/4080 [==============================] - 55s 13ms/sample - loss: 1.4613 - acc: 0.2444 - val_loss: 4.3128 - val_acc: 0.0155\n",
      "Epoch 24/25\n",
      "4080/4080 [==============================] - 55s 14ms/sample - loss: 1.4512 - acc: 0.2583 - val_loss: 4.5270 - val_acc: 0.0174\n",
      "Epoch 25/25\n",
      "4080/4080 [==============================] - 56s 14ms/sample - loss: 1.4546 - acc: 0.2529 - val_loss: 4.4683 - val_acc: 0.0155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x130b44b50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 25\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_validation, y_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = Path('../model_weights.h5').resolve()\n",
    "model.save_weights(model_file.as_posix())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
